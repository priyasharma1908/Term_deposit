{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8659c578",
   "metadata": {},
   "source": [
    "# Case Study - Term Deposit\n",
    "\n",
    "**Author:** Priya Sharma\n",
    "\n",
    "**Email:** priyasharma1908@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5c621",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932485d0",
   "metadata": {},
   "source": [
    "The objective of this report is to present the analysis and results from modeling client data to predict if client will subscribe to term deposit.\n",
    "\n",
    "We have followed the **scientific method** for this analysis and have broken down the steps as follows:\n",
    "\n",
    "* **Objective**\n",
    "\n",
    "* **Research**\n",
    "\n",
    "* **Hypothesis**\n",
    "\n",
    "* **Analysis**\n",
    "\n",
    "* **Conclusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063f4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import shap\n",
    "\n",
    "from dython.nominal import theils_u, correlation_ratio\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc3ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "term_deposit_data = pd.read_csv(\"./bank-additional-full.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce76f4c",
   "metadata": {},
   "source": [
    "## Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe25233",
   "metadata": {},
   "source": [
    "### Term Deposit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2795c",
   "metadata": {},
   "source": [
    "* A term deposit is a type of deposit account held at a financial institution where money is locked up for some set period of time.\n",
    "\n",
    "* Term deposits are usually short-term deposits with maturities ranging from one month to a few years.\n",
    "\n",
    "* Typically, term deposits offer higher interest rates than traditional liquid savings accounts, whereby customers can withdraw their money at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf9a34",
   "metadata": {},
   "source": [
    "## User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bfa99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def remove_rows_with_unknowns(df:'pd.DataFrame')->'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Function to drop rows having 'unknown' values for any variable.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe without any 'unknown' values\n",
    "    \"\"\"\n",
    "    return(df[~df.replace('unknown', np.nan).isna().sum(axis=1).astype('bool')])\n",
    "\n",
    "def min_max_scaling(train_df:'pd.DataFrame', test_df:'pd.DataFrame')->'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Function to scale numeric columns of train and test dataframes using MinMaxScaler\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe. Scaler will be built using this\n",
    "        test_df: Test dataframe. Scaler built from train would be applied on test\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe with scaled numeric columns\n",
    "    \"\"\"\n",
    "    for column in train_df.columns:\n",
    "        if(is_numeric_dtype(train_df[column])):\n",
    "            scaler = MinMaxScaler()\n",
    "            train_df[column] = scaler.fit_transform(train_df[[column]])\n",
    "            test_df[column] = scaler.transform(test_df[[column]])\n",
    "    return(train_df, test_df)\n",
    "\n",
    "def compute_correlation(df:'pd.DataFrame')->'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Function to compute correlation values as follows:\n",
    "    - Pearson correlation (numeric-numeric)\n",
    "    - Correlation Ratio (numeric-categorical)\n",
    "    - Theil's U (categorical-categorical)\n",
    "    \n",
    "    Args:\n",
    "        df: Dataset for which to compute correlation matrix\n",
    "        \n",
    "    Returns:\n",
    "        Correlation matrix as dataframe\n",
    "    \"\"\"\n",
    "    # Get list of columns\n",
    "    list_of_columns = df.columns\n",
    "    \n",
    "    # Initialize empty dataframe for correlation matrix\n",
    "    corr_df = pd.DataFrame(index=list_of_columns, columns=list_of_columns)\n",
    "    \n",
    "    # Iterate over each column\n",
    "    for i in range(len(list_of_columns)):\n",
    "        # For each column, iterate over list of columns again to get pair-wise columns\n",
    "        # Note: We are iterating over (i, j) and (j, i) separately as Theil's U is not symmetric\n",
    "        for j in range(len(list_of_columns)):\n",
    "            if is_numeric_dtype(df[list_of_columns[i]]):\n",
    "                if is_numeric_dtype(df[list_of_columns[j]]):\n",
    "                    # Case 1: Both are numeric\n",
    "                    corr_value = np.corrcoef(df[list_of_columns[i]], df[list_of_columns[j]])\n",
    "                    corr_df.loc[list_of_columns[i], list_of_columns[j]] = corr_value[0][1]\n",
    "                else:\n",
    "                    # Case 2: One is categorical\n",
    "                    corr_value = correlation_ratio(df[list_of_columns[j]], df[list_of_columns[i]])\n",
    "                    corr_df.loc[list_of_columns[i], list_of_columns[j]] = corr_value\n",
    "            elif is_numeric_dtype(df[list_of_columns[j]]):\n",
    "                if is_numeric_dtype(df[list_of_columns[i]]):\n",
    "                    # Case 1: Both are numeric\n",
    "                    corr_value = np.corrcoef(df[list_of_columns[i]], df[list_of_columns[j]])\n",
    "                    corr_df.loc[list_of_columns[i], list_of_columns[j]] = corr_value[0][1]\n",
    "                else:\n",
    "                    # Case 2: One is categorical\n",
    "                    corr_value = correlation_ratio(df[list_of_columns[i]], df[list_of_columns[j]])\n",
    "                    corr_df.loc[list_of_columns[i], list_of_columns[j]] = corr_value\n",
    "            else:\n",
    "                # Case 3: Both are categorical\n",
    "                corr_value = theils_u(df[list_of_columns[i]], df[list_of_columns[j]])\n",
    "                corr_df.loc[list_of_columns[i], list_of_columns[j]] = corr_value\n",
    "    return(pd.DataFrame(corr_df.astype('float64').round(2)))\n",
    "\n",
    "def get_model_summary(model:'sklearn.linear_model.LogisticRegression', X:'pd.DataFrame', y:'pd.Series')->'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Function to compute model summary for sklearn Logistic Regression model as sklearn does not have\n",
    "    implimentation of the same.\n",
    "    \n",
    "    Args:\n",
    "        model: sklearn.linear_model.LogitsticRegression model object\n",
    "        X: DataFrame with independent variables from training set\n",
    "        y: Series with dependent variable from training set\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing model summary\n",
    "    \"\"\"\n",
    "    # Extract model parameters\n",
    "    params = np.append(model.intercept_, model.coef_)\n",
    "    \n",
    "    # Make predictions for training set (Fitted values)\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Original dataset with column 'Constant' for intercept\n",
    "    newX = pd.DataFrame({\n",
    "        \"Constant\": np.ones(len(X))\n",
    "    }).join(pd.DataFrame(X).reset_index(drop=True))\n",
    "    \n",
    "    # Compute t stat and p value\n",
    "    MSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))\n",
    "    \n",
    "    var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = params/ sd_b\n",
    "    \n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-len(newX.columns)))) for i in ts_b]\n",
    "    \n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    params = np.round(params,4)\n",
    "    \n",
    "    # Return model summary as DataFrame\n",
    "    myDF3 = pd.DataFrame(index = newX.columns)\n",
    "    myDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"],myDF3[\"Probabilities\"] = [params,sd_b,ts_b,p_values]\n",
    "    \n",
    "    return(myDF3)\n",
    "\n",
    "def missing_value_mode_treatment(train_df:'pd.DataFrame', test_df:'pd.DataFrame')->'tuple':\n",
    "    \"\"\"\n",
    "    Function to impute missing values using mode of the variable.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataset\n",
    "        test_df: Test dataset\n",
    "        \n",
    "    Returns:\n",
    "        Tuple with first element as the treated training dataset and second element as treated test dataset\n",
    "    \"\"\"\n",
    "    # Get columns with unknown values\n",
    "    unknown_count = train_df.replace('unknown', np.nan).isna().sum(axis=0)\n",
    "    unknown_columns = list(unknown_count[unknown_count > 0].index)\n",
    "    \n",
    "    # For each column\n",
    "    for column in unknown_columns:\n",
    "        # Get mode\n",
    "        mode_value = train_df[column].mode()[0]\n",
    "        \n",
    "        # Update unknown with mode in training set\n",
    "        train_df[column] = train_df[column].replace(\"unknown\", np.nan).fillna(mode_value)\n",
    "        \n",
    "        # Update unknown with model in test set\n",
    "        test_df[column] = test_df[column].replace(\"unknown\", np.nan).fillna(mode_value)\n",
    "    return(train_df, test_df)\n",
    "\n",
    "def remove_outliers_iqr(train_x:'pd.DataFrame', test_x:'pd.DataFrame')->'tuple':\n",
    "    \"\"\"\n",
    "    Function to remove outliers using Inter Quartile Range method\n",
    "    \n",
    "    Args:\n",
    "        train_x: Training dataset\n",
    "        test_x: Test dataset\n",
    "        \n",
    "    Returns:\n",
    "        Tuple with first element as the treated training dataset and second element as treated test dataset\n",
    "    \"\"\"\n",
    "    # Compute Q1 and Q3\n",
    "    q1 = np.quantile(train_x, 0.25)\n",
    "    q3 = np.quantile(train_x, 0.75)\n",
    "    \n",
    "    # Compute IQR\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Get upper and lower limits\n",
    "    lower_limit = q1 - 1.5 * iqr\n",
    "    upper_limit = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Cap outliers to upper and lower limits\n",
    "    train_x[train_x < lower_limit] = lower_limit\n",
    "    train_x[train_x > upper_limit] = upper_limit\n",
    "    test_x[test_x < lower_limit] = lower_limit\n",
    "    test_x[test_x > upper_limit] = upper_limit\n",
    "    return(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ff0df",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360ea68",
   "metadata": {},
   "source": [
    "The hypothesis for this analysis is as follows:\n",
    "\n",
    "**A combination of CLIENT DATA, LAST CONTACT DATA and ADDITIONAL ATTRIBUTES can be used to predict if client will subscribe to a term deposit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f203d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
